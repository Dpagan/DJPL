{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YTfbltWs4jaf"
   },
   "source": [
    "# ReneWind\n",
    "\n",
    "Renewable energy sources play an increasingly important role in the global energy mix, as the effort to reduce the environmental impact of energy production increases.\n",
    "\n",
    "Out of all the renewable energy alternatives, wind energy is one of the most developed technologies worldwide. The U.S Department of Energy has put together a guide to achieving operational efficiency using predictive maintenance practices.\n",
    "\n",
    "Predictive maintenance uses sensor information and analysis methods to measure and predict degradation and future component capability. The idea behind predictive maintenance is that failure patterns are predictable and if component failure can be predicted accurately and the component is replaced before it fails, the costs of operation and maintenance will be much lower.\n",
    "\n",
    "The sensors fitted across different machines involved in the process of energy generation collect data related to various environmental factors (temperature, humidity, wind speed, etc.) and additional features related to various parts of the wind turbine (gearbox, tower, blades, break, etc.). \n",
    "\n",
    "\n",
    "\n",
    "## Objective\n",
    "“ReneWind” is a company working on improving the machinery/processes involved in the production of wind energy using machine learning and has collected data of generator failure of wind turbines using sensors. They have shared a ciphered version of the data, as the data collected through sensors is confidential (the type of data collected varies with companies). Data has 40 predictors, 40000 observations in the training set and 10000 in the test set.\n",
    "\n",
    "The objective is to build various classification models, tune them and find the best one that will help identify failures so that the generator could be repaired before failing/breaking and the overall maintenance cost of the generators can be brought down. \n",
    "\n",
    "“1” in the target variables should be considered as “failure” and “0” will represent “No failure”.\n",
    "\n",
    "The nature of predictions made by the classification model will translate as follows:\n",
    "\n",
    "- True positives (TP) are failures correctly predicted by the model.\n",
    "- False negatives (FN) are real failures in a wind turbine where there is no detection by model. \n",
    "- False positives (FP) are detections in a wind turbine where there is no failure. \n",
    "\n",
    "So, the maintenance cost associated with the model would be:\n",
    "\n",
    "**Maintenance cost** = `TP*(Repair cost) + FN*(Replacement cost) + FP*(Inspection cost)`\n",
    "where,\n",
    "\n",
    "- `Replacement cost = $40,000`\n",
    "- `Repair cost = $15,000`\n",
    "- `Inspection cost = $5,000`\n",
    "\n",
    "Here the objective is to reduce the maintenance cost so, we want a metric that could reduce the maintenance cost.\n",
    "\n",
    "- The minimum possible maintenance cost  =  `Actual failures*(Repair cost) = (TP + FN)*(Repair cost)`\n",
    "- The maintenance cost associated with model = `TP*(Repair cost) + FN*(Replacement cost) + FP*(Inspection cost)`\n",
    "\n",
    "So, we will try to maximize the ratio of minimum possible maintenance cost and the maintenance cost associated with the model.\n",
    "\n",
    "The value of this ratio will lie between 0 and 1, the ratio will be 1 only when the maintenance cost associated with the model will be equal to the minimum possible maintenance cost.\n",
    "\n",
    "## Data Description\n",
    "- The data provided is a transformed version of original data which was collected using sensors.\n",
    "- Train.csv - To be used for training and tuning of models. \n",
    "- Test.csv - To be used only for testing the performance of the final best model.\n",
    "- Both the datasets consist of 40 predictor variables and 1 target variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jjFpJBnb4jak"
   },
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "83D17_Wl4jal"
   },
   "outputs": [],
   "source": [
    "# To help with reading and manipulating data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# To help with data visualization\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# To be used for missing value imputation\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "\n",
    "# To help with model building\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import (\n",
    "    AdaBoostClassifier,\n",
    "    GradientBoostingClassifier,\n",
    "    RandomForestClassifier,\n",
    "    BaggingClassifier,\n",
    ")\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# To get different metric scores, and split data\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.metrics import (\n",
    "    f1_score,\n",
    "    accuracy_score,\n",
    "    recall_score,\n",
    "    precision_score,\n",
    "    confusion_matrix,\n",
    "    roc_auc_score,\n",
    "    plot_confusion_matrix,\n",
    ")\n",
    "\n",
    "# To be used for data scaling and one hot encoding\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\n",
    "\n",
    "# To be used for tuning the model\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "# To be used for creating pipelines and personalizing them\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# To define maximum number of columns to be displayed in a dataframe\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "# To supress scientific notations for a dataframe\n",
    "pd.set_option(\"display.float_format\", lambda x: \"%.3f\" % x)\n",
    "\n",
    "# To supress warnings\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vqF4q7G94jam"
   },
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "oJnKoHy14jam"
   },
   "outputs": [],
   "source": [
    "RW = pd.read_csv(\"Train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DVCj6_DD4jan"
   },
   "source": [
    "## EDA and insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "W91y_f944jan"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000, 41)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the number of rows and columns in the data\n",
    "RW.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data has 40000 rows and 41 columns.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We create a copy of the data\n",
    "data = RW.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>V11</th>\n",
       "      <th>V12</th>\n",
       "      <th>V13</th>\n",
       "      <th>V14</th>\n",
       "      <th>V15</th>\n",
       "      <th>V16</th>\n",
       "      <th>V17</th>\n",
       "      <th>V18</th>\n",
       "      <th>V19</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>V29</th>\n",
       "      <th>V30</th>\n",
       "      <th>V31</th>\n",
       "      <th>V32</th>\n",
       "      <th>V33</th>\n",
       "      <th>V34</th>\n",
       "      <th>V35</th>\n",
       "      <th>V36</th>\n",
       "      <th>V37</th>\n",
       "      <th>V38</th>\n",
       "      <th>V39</th>\n",
       "      <th>V40</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-4.465</td>\n",
       "      <td>-4.679</td>\n",
       "      <td>3.102</td>\n",
       "      <td>0.506</td>\n",
       "      <td>-0.221</td>\n",
       "      <td>-2.033</td>\n",
       "      <td>-2.911</td>\n",
       "      <td>0.051</td>\n",
       "      <td>-1.522</td>\n",
       "      <td>3.762</td>\n",
       "      <td>-5.715</td>\n",
       "      <td>0.736</td>\n",
       "      <td>0.981</td>\n",
       "      <td>1.418</td>\n",
       "      <td>-3.376</td>\n",
       "      <td>-3.047</td>\n",
       "      <td>0.306</td>\n",
       "      <td>2.914</td>\n",
       "      <td>2.270</td>\n",
       "      <td>4.395</td>\n",
       "      <td>-2.388</td>\n",
       "      <td>0.646</td>\n",
       "      <td>-1.191</td>\n",
       "      <td>3.133</td>\n",
       "      <td>0.665</td>\n",
       "      <td>-2.511</td>\n",
       "      <td>-0.037</td>\n",
       "      <td>0.726</td>\n",
       "      <td>-3.982</td>\n",
       "      <td>-1.073</td>\n",
       "      <td>1.667</td>\n",
       "      <td>3.060</td>\n",
       "      <td>-1.690</td>\n",
       "      <td>2.846</td>\n",
       "      <td>2.235</td>\n",
       "      <td>6.667</td>\n",
       "      <td>0.444</td>\n",
       "      <td>-2.369</td>\n",
       "      <td>2.951</td>\n",
       "      <td>-3.480</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-2.910</td>\n",
       "      <td>-2.569</td>\n",
       "      <td>4.109</td>\n",
       "      <td>1.317</td>\n",
       "      <td>-1.621</td>\n",
       "      <td>-3.827</td>\n",
       "      <td>-1.617</td>\n",
       "      <td>0.669</td>\n",
       "      <td>0.387</td>\n",
       "      <td>0.854</td>\n",
       "      <td>-6.353</td>\n",
       "      <td>4.272</td>\n",
       "      <td>3.162</td>\n",
       "      <td>0.258</td>\n",
       "      <td>-3.547</td>\n",
       "      <td>-4.285</td>\n",
       "      <td>2.897</td>\n",
       "      <td>1.508</td>\n",
       "      <td>3.668</td>\n",
       "      <td>7.124</td>\n",
       "      <td>-4.096</td>\n",
       "      <td>1.015</td>\n",
       "      <td>-0.970</td>\n",
       "      <td>-0.968</td>\n",
       "      <td>2.064</td>\n",
       "      <td>-1.646</td>\n",
       "      <td>0.427</td>\n",
       "      <td>0.735</td>\n",
       "      <td>-4.470</td>\n",
       "      <td>-2.772</td>\n",
       "      <td>-2.505</td>\n",
       "      <td>-3.783</td>\n",
       "      <td>-6.823</td>\n",
       "      <td>4.909</td>\n",
       "      <td>0.482</td>\n",
       "      <td>5.338</td>\n",
       "      <td>2.381</td>\n",
       "      <td>-3.128</td>\n",
       "      <td>3.527</td>\n",
       "      <td>-3.020</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.284</td>\n",
       "      <td>5.105</td>\n",
       "      <td>6.092</td>\n",
       "      <td>2.640</td>\n",
       "      <td>-1.041</td>\n",
       "      <td>1.308</td>\n",
       "      <td>-1.876</td>\n",
       "      <td>-9.582</td>\n",
       "      <td>3.470</td>\n",
       "      <td>0.763</td>\n",
       "      <td>-2.573</td>\n",
       "      <td>-3.350</td>\n",
       "      <td>-0.595</td>\n",
       "      <td>-5.247</td>\n",
       "      <td>-4.310</td>\n",
       "      <td>-16.232</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>2.318</td>\n",
       "      <td>5.942</td>\n",
       "      <td>-3.858</td>\n",
       "      <td>-11.599</td>\n",
       "      <td>4.021</td>\n",
       "      <td>-6.281</td>\n",
       "      <td>4.633</td>\n",
       "      <td>0.930</td>\n",
       "      <td>6.280</td>\n",
       "      <td>0.851</td>\n",
       "      <td>0.269</td>\n",
       "      <td>-2.206</td>\n",
       "      <td>-1.329</td>\n",
       "      <td>-2.399</td>\n",
       "      <td>-3.098</td>\n",
       "      <td>2.690</td>\n",
       "      <td>-1.643</td>\n",
       "      <td>7.566</td>\n",
       "      <td>-3.198</td>\n",
       "      <td>-3.496</td>\n",
       "      <td>8.105</td>\n",
       "      <td>0.562</td>\n",
       "      <td>-4.227</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.366</td>\n",
       "      <td>3.653</td>\n",
       "      <td>0.910</td>\n",
       "      <td>-1.368</td>\n",
       "      <td>0.332</td>\n",
       "      <td>2.359</td>\n",
       "      <td>0.733</td>\n",
       "      <td>-4.332</td>\n",
       "      <td>0.566</td>\n",
       "      <td>-0.101</td>\n",
       "      <td>1.914</td>\n",
       "      <td>-0.951</td>\n",
       "      <td>-1.255</td>\n",
       "      <td>-2.707</td>\n",
       "      <td>0.193</td>\n",
       "      <td>-4.769</td>\n",
       "      <td>-2.205</td>\n",
       "      <td>0.908</td>\n",
       "      <td>0.757</td>\n",
       "      <td>-5.834</td>\n",
       "      <td>-3.065</td>\n",
       "      <td>1.597</td>\n",
       "      <td>-1.757</td>\n",
       "      <td>1.766</td>\n",
       "      <td>-0.267</td>\n",
       "      <td>3.625</td>\n",
       "      <td>1.500</td>\n",
       "      <td>-0.586</td>\n",
       "      <td>0.783</td>\n",
       "      <td>-0.201</td>\n",
       "      <td>0.025</td>\n",
       "      <td>-1.795</td>\n",
       "      <td>3.033</td>\n",
       "      <td>-2.468</td>\n",
       "      <td>1.895</td>\n",
       "      <td>-2.298</td>\n",
       "      <td>-1.731</td>\n",
       "      <td>5.909</td>\n",
       "      <td>-0.386</td>\n",
       "      <td>0.616</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-3.832</td>\n",
       "      <td>-5.824</td>\n",
       "      <td>0.634</td>\n",
       "      <td>-2.419</td>\n",
       "      <td>-1.774</td>\n",
       "      <td>1.017</td>\n",
       "      <td>-2.099</td>\n",
       "      <td>-3.173</td>\n",
       "      <td>-2.082</td>\n",
       "      <td>5.393</td>\n",
       "      <td>-0.771</td>\n",
       "      <td>1.107</td>\n",
       "      <td>1.144</td>\n",
       "      <td>0.943</td>\n",
       "      <td>-3.164</td>\n",
       "      <td>-4.248</td>\n",
       "      <td>-4.039</td>\n",
       "      <td>3.689</td>\n",
       "      <td>3.311</td>\n",
       "      <td>1.059</td>\n",
       "      <td>-2.143</td>\n",
       "      <td>1.650</td>\n",
       "      <td>-1.661</td>\n",
       "      <td>1.680</td>\n",
       "      <td>-0.451</td>\n",
       "      <td>-4.551</td>\n",
       "      <td>3.739</td>\n",
       "      <td>1.134</td>\n",
       "      <td>-2.034</td>\n",
       "      <td>0.841</td>\n",
       "      <td>-1.600</td>\n",
       "      <td>-0.257</td>\n",
       "      <td>0.804</td>\n",
       "      <td>4.086</td>\n",
       "      <td>2.292</td>\n",
       "      <td>5.361</td>\n",
       "      <td>0.352</td>\n",
       "      <td>2.940</td>\n",
       "      <td>3.839</td>\n",
       "      <td>-4.309</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.618</td>\n",
       "      <td>1.888</td>\n",
       "      <td>7.046</td>\n",
       "      <td>-1.147</td>\n",
       "      <td>0.083</td>\n",
       "      <td>-1.530</td>\n",
       "      <td>0.207</td>\n",
       "      <td>-2.494</td>\n",
       "      <td>0.345</td>\n",
       "      <td>2.119</td>\n",
       "      <td>-3.053</td>\n",
       "      <td>0.460</td>\n",
       "      <td>2.705</td>\n",
       "      <td>-0.636</td>\n",
       "      <td>-0.454</td>\n",
       "      <td>-3.174</td>\n",
       "      <td>-3.404</td>\n",
       "      <td>-1.282</td>\n",
       "      <td>1.582</td>\n",
       "      <td>-1.952</td>\n",
       "      <td>-3.517</td>\n",
       "      <td>-1.206</td>\n",
       "      <td>-5.628</td>\n",
       "      <td>-1.818</td>\n",
       "      <td>2.124</td>\n",
       "      <td>5.295</td>\n",
       "      <td>4.748</td>\n",
       "      <td>-2.309</td>\n",
       "      <td>-3.963</td>\n",
       "      <td>-6.029</td>\n",
       "      <td>4.949</td>\n",
       "      <td>-3.584</td>\n",
       "      <td>-2.577</td>\n",
       "      <td>1.364</td>\n",
       "      <td>0.623</td>\n",
       "      <td>5.550</td>\n",
       "      <td>-1.527</td>\n",
       "      <td>0.139</td>\n",
       "      <td>3.101</td>\n",
       "      <td>-1.277</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.111</td>\n",
       "      <td>3.872</td>\n",
       "      <td>-3.758</td>\n",
       "      <td>-2.983</td>\n",
       "      <td>3.793</td>\n",
       "      <td>0.545</td>\n",
       "      <td>0.205</td>\n",
       "      <td>4.849</td>\n",
       "      <td>-1.855</td>\n",
       "      <td>-6.220</td>\n",
       "      <td>1.998</td>\n",
       "      <td>4.724</td>\n",
       "      <td>0.709</td>\n",
       "      <td>-1.989</td>\n",
       "      <td>-2.633</td>\n",
       "      <td>4.184</td>\n",
       "      <td>2.245</td>\n",
       "      <td>3.734</td>\n",
       "      <td>-6.313</td>\n",
       "      <td>-5.380</td>\n",
       "      <td>-0.887</td>\n",
       "      <td>2.062</td>\n",
       "      <td>9.446</td>\n",
       "      <td>4.490</td>\n",
       "      <td>-3.945</td>\n",
       "      <td>4.582</td>\n",
       "      <td>-8.780</td>\n",
       "      <td>-3.383</td>\n",
       "      <td>5.107</td>\n",
       "      <td>6.788</td>\n",
       "      <td>2.044</td>\n",
       "      <td>8.266</td>\n",
       "      <td>6.629</td>\n",
       "      <td>-10.069</td>\n",
       "      <td>1.223</td>\n",
       "      <td>-3.230</td>\n",
       "      <td>1.687</td>\n",
       "      <td>-2.164</td>\n",
       "      <td>-3.645</td>\n",
       "      <td>6.510</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.160</td>\n",
       "      <td>-4.234</td>\n",
       "      <td>-0.264</td>\n",
       "      <td>-5.477</td>\n",
       "      <td>-0.191</td>\n",
       "      <td>-0.356</td>\n",
       "      <td>-0.134</td>\n",
       "      <td>4.067</td>\n",
       "      <td>-3.859</td>\n",
       "      <td>1.692</td>\n",
       "      <td>0.138</td>\n",
       "      <td>3.975</td>\n",
       "      <td>0.673</td>\n",
       "      <td>1.878</td>\n",
       "      <td>0.764</td>\n",
       "      <td>4.236</td>\n",
       "      <td>-2.129</td>\n",
       "      <td>2.348</td>\n",
       "      <td>-2.147</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>0.386</td>\n",
       "      <td>1.011</td>\n",
       "      <td>3.419</td>\n",
       "      <td>0.996</td>\n",
       "      <td>0.061</td>\n",
       "      <td>-3.037</td>\n",
       "      <td>1.788</td>\n",
       "      <td>-1.727</td>\n",
       "      <td>0.308</td>\n",
       "      <td>1.902</td>\n",
       "      <td>4.666</td>\n",
       "      <td>3.227</td>\n",
       "      <td>0.629</td>\n",
       "      <td>-1.549</td>\n",
       "      <td>1.322</td>\n",
       "      <td>5.461</td>\n",
       "      <td>1.109</td>\n",
       "      <td>-3.870</td>\n",
       "      <td>0.274</td>\n",
       "      <td>2.806</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-3.645</td>\n",
       "      <td>-1.041</td>\n",
       "      <td>2.425</td>\n",
       "      <td>1.465</td>\n",
       "      <td>-0.422</td>\n",
       "      <td>-1.516</td>\n",
       "      <td>-0.955</td>\n",
       "      <td>1.161</td>\n",
       "      <td>-2.024</td>\n",
       "      <td>1.213</td>\n",
       "      <td>-0.148</td>\n",
       "      <td>2.617</td>\n",
       "      <td>0.939</td>\n",
       "      <td>-0.700</td>\n",
       "      <td>-1.601</td>\n",
       "      <td>1.007</td>\n",
       "      <td>-0.796</td>\n",
       "      <td>-0.817</td>\n",
       "      <td>3.541</td>\n",
       "      <td>1.064</td>\n",
       "      <td>-0.984</td>\n",
       "      <td>-1.400</td>\n",
       "      <td>-1.364</td>\n",
       "      <td>1.688</td>\n",
       "      <td>-1.626</td>\n",
       "      <td>0.553</td>\n",
       "      <td>-2.372</td>\n",
       "      <td>-0.185</td>\n",
       "      <td>1.244</td>\n",
       "      <td>2.561</td>\n",
       "      <td>0.801</td>\n",
       "      <td>2.319</td>\n",
       "      <td>2.079</td>\n",
       "      <td>2.214</td>\n",
       "      <td>3.288</td>\n",
       "      <td>1.367</td>\n",
       "      <td>-0.304</td>\n",
       "      <td>-0.381</td>\n",
       "      <td>1.090</td>\n",
       "      <td>-2.592</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.185</td>\n",
       "      <td>-4.721</td>\n",
       "      <td>0.865</td>\n",
       "      <td>-3.079</td>\n",
       "      <td>-2.227</td>\n",
       "      <td>-1.282</td>\n",
       "      <td>-0.805</td>\n",
       "      <td>3.290</td>\n",
       "      <td>-1.568</td>\n",
       "      <td>0.750</td>\n",
       "      <td>0.529</td>\n",
       "      <td>3.221</td>\n",
       "      <td>2.945</td>\n",
       "      <td>1.724</td>\n",
       "      <td>-0.923</td>\n",
       "      <td>2.535</td>\n",
       "      <td>-1.697</td>\n",
       "      <td>0.677</td>\n",
       "      <td>-0.246</td>\n",
       "      <td>2.748</td>\n",
       "      <td>-1.165</td>\n",
       "      <td>0.248</td>\n",
       "      <td>1.161</td>\n",
       "      <td>-2.850</td>\n",
       "      <td>0.503</td>\n",
       "      <td>-3.532</td>\n",
       "      <td>1.861</td>\n",
       "      <td>-1.465</td>\n",
       "      <td>0.874</td>\n",
       "      <td>2.418</td>\n",
       "      <td>0.939</td>\n",
       "      <td>-0.545</td>\n",
       "      <td>-0.763</td>\n",
       "      <td>0.816</td>\n",
       "      <td>1.889</td>\n",
       "      <td>3.624</td>\n",
       "      <td>1.556</td>\n",
       "      <td>-5.433</td>\n",
       "      <td>0.679</td>\n",
       "      <td>0.465</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      V1     V2     V3     V4     V5     V6     V7     V8     V9    V10  \\\n",
       "0 -4.465 -4.679  3.102  0.506 -0.221 -2.033 -2.911  0.051 -1.522  3.762   \n",
       "1 -2.910 -2.569  4.109  1.317 -1.621 -3.827 -1.617  0.669  0.387  0.854   \n",
       "2  4.284  5.105  6.092  2.640 -1.041  1.308 -1.876 -9.582  3.470  0.763   \n",
       "3  3.366  3.653  0.910 -1.368  0.332  2.359  0.733 -4.332  0.566 -0.101   \n",
       "4 -3.832 -5.824  0.634 -2.419 -1.774  1.017 -2.099 -3.173 -2.082  5.393   \n",
       "5  1.618  1.888  7.046 -1.147  0.083 -1.530  0.207 -2.494  0.345  2.119   \n",
       "6 -0.111  3.872 -3.758 -2.983  3.793  0.545  0.205  4.849 -1.855 -6.220   \n",
       "7  0.160 -4.234 -0.264 -5.477 -0.191 -0.356 -0.134  4.067 -3.859  1.692   \n",
       "8 -3.645 -1.041  2.425  1.465 -0.422 -1.516 -0.955  1.161 -2.024  1.213   \n",
       "9 -0.185 -4.721  0.865 -3.079 -2.227 -1.282 -0.805  3.290 -1.568  0.750   \n",
       "\n",
       "     V11    V12    V13    V14    V15     V16    V17    V18    V19    V20  \\\n",
       "0 -5.715  0.736  0.981  1.418 -3.376  -3.047  0.306  2.914  2.270  4.395   \n",
       "1 -6.353  4.272  3.162  0.258 -3.547  -4.285  2.897  1.508  3.668  7.124   \n",
       "2 -2.573 -3.350 -0.595 -5.247 -4.310 -16.232 -1.000  2.318  5.942 -3.858   \n",
       "3  1.914 -0.951 -1.255 -2.707  0.193  -4.769 -2.205  0.908  0.757 -5.834   \n",
       "4 -0.771  1.107  1.144  0.943 -3.164  -4.248 -4.039  3.689  3.311  1.059   \n",
       "5 -3.053  0.460  2.705 -0.636 -0.454  -3.174 -3.404 -1.282  1.582 -1.952   \n",
       "6  1.998  4.724  0.709 -1.989 -2.633   4.184  2.245  3.734 -6.313 -5.380   \n",
       "7  0.138  3.975  0.673  1.878  0.764   4.236 -2.129  2.348 -2.147 -0.982   \n",
       "8 -0.148  2.617  0.939 -0.700 -1.601   1.007 -0.796 -0.817  3.541  1.064   \n",
       "9  0.529  3.221  2.945  1.724 -0.923   2.535 -1.697  0.677 -0.246  2.748   \n",
       "\n",
       "      V21    V22    V23    V24    V25    V26    V27    V28    V29    V30  \\\n",
       "0  -2.388  0.646 -1.191  3.133  0.665 -2.511 -0.037  0.726 -3.982 -1.073   \n",
       "1  -4.096  1.015 -0.970 -0.968  2.064 -1.646  0.427  0.735 -4.470 -2.772   \n",
       "2 -11.599  4.021 -6.281  4.633  0.930  6.280  0.851  0.269 -2.206 -1.329   \n",
       "3  -3.065  1.597 -1.757  1.766 -0.267  3.625  1.500 -0.586  0.783 -0.201   \n",
       "4  -2.143  1.650 -1.661  1.680 -0.451 -4.551  3.739  1.134 -2.034  0.841   \n",
       "5  -3.517 -1.206 -5.628 -1.818  2.124  5.295  4.748 -2.309 -3.963 -6.029   \n",
       "6  -0.887  2.062  9.446  4.490 -3.945  4.582 -8.780 -3.383  5.107  6.788   \n",
       "7   0.386  1.011  3.419  0.996  0.061 -3.037  1.788 -1.727  0.308  1.902   \n",
       "8  -0.984 -1.400 -1.364  1.688 -1.626  0.553 -2.372 -0.185  1.244  2.561   \n",
       "9  -1.165  0.248  1.161 -2.850  0.503 -3.532  1.861 -1.465  0.874  2.418   \n",
       "\n",
       "     V31    V32    V33     V34   V35    V36    V37    V38    V39    V40  \\\n",
       "0  1.667  3.060 -1.690   2.846 2.235  6.667  0.444 -2.369  2.951 -3.480   \n",
       "1 -2.505 -3.783 -6.823   4.909 0.482  5.338  2.381 -3.128  3.527 -3.020   \n",
       "2 -2.399 -3.098  2.690  -1.643 7.566 -3.198 -3.496  8.105  0.562 -4.227   \n",
       "3  0.025 -1.795  3.033  -2.468 1.895 -2.298 -1.731  5.909 -0.386  0.616   \n",
       "4 -1.600 -0.257  0.804   4.086 2.292  5.361  0.352  2.940  3.839 -4.309   \n",
       "5  4.949 -3.584 -2.577   1.364 0.623  5.550 -1.527  0.139  3.101 -1.277   \n",
       "6  2.044  8.266  6.629 -10.069 1.223 -3.230  1.687 -2.164 -3.645  6.510   \n",
       "7  4.666  3.227  0.629  -1.549 1.322  5.461  1.109 -3.870  0.274  2.806   \n",
       "8  0.801  2.319  2.079   2.214 3.288  1.367 -0.304 -0.381  1.090 -2.592   \n",
       "9  0.939 -0.545 -0.763   0.816 1.889  3.624  1.556 -5.433  0.679  0.465   \n",
       "\n",
       "   Target  \n",
       "0       0  \n",
       "1       0  \n",
       "2       0  \n",
       "3       0  \n",
       "4       0  \n",
       "5       0  \n",
       "6       0  \n",
       "7       0  \n",
       "8       0  \n",
       "9       0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's view the first 10 rows of the data\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>V11</th>\n",
       "      <th>V12</th>\n",
       "      <th>V13</th>\n",
       "      <th>V14</th>\n",
       "      <th>V15</th>\n",
       "      <th>V16</th>\n",
       "      <th>V17</th>\n",
       "      <th>V18</th>\n",
       "      <th>V19</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>V29</th>\n",
       "      <th>V30</th>\n",
       "      <th>V31</th>\n",
       "      <th>V32</th>\n",
       "      <th>V33</th>\n",
       "      <th>V34</th>\n",
       "      <th>V35</th>\n",
       "      <th>V36</th>\n",
       "      <th>V37</th>\n",
       "      <th>V38</th>\n",
       "      <th>V39</th>\n",
       "      <th>V40</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>39990</th>\n",
       "      <td>1.850</td>\n",
       "      <td>3.606</td>\n",
       "      <td>0.979</td>\n",
       "      <td>3.262</td>\n",
       "      <td>0.353</td>\n",
       "      <td>-0.267</td>\n",
       "      <td>-0.522</td>\n",
       "      <td>-3.015</td>\n",
       "      <td>1.819</td>\n",
       "      <td>-0.391</td>\n",
       "      <td>-1.404</td>\n",
       "      <td>-1.286</td>\n",
       "      <td>0.050</td>\n",
       "      <td>-2.083</td>\n",
       "      <td>-2.609</td>\n",
       "      <td>-6.315</td>\n",
       "      <td>0.591</td>\n",
       "      <td>0.077</td>\n",
       "      <td>2.901</td>\n",
       "      <td>-2.107</td>\n",
       "      <td>-5.172</td>\n",
       "      <td>2.504</td>\n",
       "      <td>0.272</td>\n",
       "      <td>4.264</td>\n",
       "      <td>-0.816</td>\n",
       "      <td>3.828</td>\n",
       "      <td>-1.985</td>\n",
       "      <td>0.466</td>\n",
       "      <td>-1.774</td>\n",
       "      <td>-0.034</td>\n",
       "      <td>-1.432</td>\n",
       "      <td>2.894</td>\n",
       "      <td>1.355</td>\n",
       "      <td>-1.563</td>\n",
       "      <td>3.778</td>\n",
       "      <td>-3.700</td>\n",
       "      <td>-2.162</td>\n",
       "      <td>3.273</td>\n",
       "      <td>-1.031</td>\n",
       "      <td>-2.999</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39991</th>\n",
       "      <td>-2.688</td>\n",
       "      <td>-2.626</td>\n",
       "      <td>-2.364</td>\n",
       "      <td>-3.041</td>\n",
       "      <td>-0.649</td>\n",
       "      <td>1.083</td>\n",
       "      <td>-0.171</td>\n",
       "      <td>-0.444</td>\n",
       "      <td>-1.568</td>\n",
       "      <td>0.773</td>\n",
       "      <td>1.260</td>\n",
       "      <td>3.761</td>\n",
       "      <td>1.152</td>\n",
       "      <td>0.048</td>\n",
       "      <td>-1.854</td>\n",
       "      <td>-0.476</td>\n",
       "      <td>-1.188</td>\n",
       "      <td>3.130</td>\n",
       "      <td>0.222</td>\n",
       "      <td>0.149</td>\n",
       "      <td>0.107</td>\n",
       "      <td>1.543</td>\n",
       "      <td>2.409</td>\n",
       "      <td>-0.166</td>\n",
       "      <td>-1.128</td>\n",
       "      <td>-3.146</td>\n",
       "      <td>0.881</td>\n",
       "      <td>0.472</td>\n",
       "      <td>0.623</td>\n",
       "      <td>2.001</td>\n",
       "      <td>-3.543</td>\n",
       "      <td>-1.826</td>\n",
       "      <td>0.128</td>\n",
       "      <td>1.531</td>\n",
       "      <td>-0.967</td>\n",
       "      <td>1.861</td>\n",
       "      <td>2.256</td>\n",
       "      <td>2.277</td>\n",
       "      <td>1.990</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39992</th>\n",
       "      <td>8.061</td>\n",
       "      <td>7.361</td>\n",
       "      <td>4.239</td>\n",
       "      <td>6.525</td>\n",
       "      <td>-1.838</td>\n",
       "      <td>-1.866</td>\n",
       "      <td>-0.113</td>\n",
       "      <td>-4.718</td>\n",
       "      <td>6.287</td>\n",
       "      <td>-3.393</td>\n",
       "      <td>-3.122</td>\n",
       "      <td>-2.927</td>\n",
       "      <td>0.722</td>\n",
       "      <td>-3.996</td>\n",
       "      <td>-2.406</td>\n",
       "      <td>-12.387</td>\n",
       "      <td>3.764</td>\n",
       "      <td>-2.313</td>\n",
       "      <td>5.224</td>\n",
       "      <td>0.209</td>\n",
       "      <td>-10.672</td>\n",
       "      <td>3.958</td>\n",
       "      <td>-2.357</td>\n",
       "      <td>1.370</td>\n",
       "      <td>2.184</td>\n",
       "      <td>6.374</td>\n",
       "      <td>-1.096</td>\n",
       "      <td>0.573</td>\n",
       "      <td>-3.013</td>\n",
       "      <td>-2.412</td>\n",
       "      <td>-4.315</td>\n",
       "      <td>-3.117</td>\n",
       "      <td>-2.818</td>\n",
       "      <td>-1.810</td>\n",
       "      <td>5.792</td>\n",
       "      <td>-8.113</td>\n",
       "      <td>-2.931</td>\n",
       "      <td>2.015</td>\n",
       "      <td>-2.773</td>\n",
       "      <td>-3.375</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39993</th>\n",
       "      <td>2.890</td>\n",
       "      <td>2.483</td>\n",
       "      <td>5.644</td>\n",
       "      <td>0.937</td>\n",
       "      <td>-1.381</td>\n",
       "      <td>0.412</td>\n",
       "      <td>-1.593</td>\n",
       "      <td>-5.762</td>\n",
       "      <td>2.150</td>\n",
       "      <td>0.272</td>\n",
       "      <td>-2.095</td>\n",
       "      <td>-1.526</td>\n",
       "      <td>0.072</td>\n",
       "      <td>-3.540</td>\n",
       "      <td>-2.762</td>\n",
       "      <td>-10.632</td>\n",
       "      <td>-0.495</td>\n",
       "      <td>1.720</td>\n",
       "      <td>3.872</td>\n",
       "      <td>-1.210</td>\n",
       "      <td>-8.222</td>\n",
       "      <td>2.121</td>\n",
       "      <td>-5.492</td>\n",
       "      <td>1.452</td>\n",
       "      <td>1.450</td>\n",
       "      <td>3.685</td>\n",
       "      <td>1.077</td>\n",
       "      <td>-0.384</td>\n",
       "      <td>-0.839</td>\n",
       "      <td>-0.748</td>\n",
       "      <td>-1.089</td>\n",
       "      <td>-4.159</td>\n",
       "      <td>1.181</td>\n",
       "      <td>-0.742</td>\n",
       "      <td>5.369</td>\n",
       "      <td>-0.693</td>\n",
       "      <td>-1.669</td>\n",
       "      <td>3.660</td>\n",
       "      <td>0.820</td>\n",
       "      <td>-1.987</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39994</th>\n",
       "      <td>2.274</td>\n",
       "      <td>4.197</td>\n",
       "      <td>2.346</td>\n",
       "      <td>1.343</td>\n",
       "      <td>1.889</td>\n",
       "      <td>-1.749</td>\n",
       "      <td>1.382</td>\n",
       "      <td>-0.209</td>\n",
       "      <td>1.099</td>\n",
       "      <td>-0.876</td>\n",
       "      <td>-4.681</td>\n",
       "      <td>0.347</td>\n",
       "      <td>-1.105</td>\n",
       "      <td>-0.692</td>\n",
       "      <td>1.752</td>\n",
       "      <td>-1.806</td>\n",
       "      <td>3.145</td>\n",
       "      <td>-0.838</td>\n",
       "      <td>-0.199</td>\n",
       "      <td>-0.510</td>\n",
       "      <td>-0.524</td>\n",
       "      <td>0.312</td>\n",
       "      <td>-0.351</td>\n",
       "      <td>1.324</td>\n",
       "      <td>2.252</td>\n",
       "      <td>3.851</td>\n",
       "      <td>0.482</td>\n",
       "      <td>0.200</td>\n",
       "      <td>-4.472</td>\n",
       "      <td>-6.366</td>\n",
       "      <td>2.263</td>\n",
       "      <td>-1.375</td>\n",
       "      <td>-5.227</td>\n",
       "      <td>-0.186</td>\n",
       "      <td>-2.636</td>\n",
       "      <td>1.344</td>\n",
       "      <td>-0.381</td>\n",
       "      <td>0.589</td>\n",
       "      <td>0.340</td>\n",
       "      <td>1.232</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39995</th>\n",
       "      <td>-3.897</td>\n",
       "      <td>-3.942</td>\n",
       "      <td>-0.351</td>\n",
       "      <td>-2.417</td>\n",
       "      <td>1.108</td>\n",
       "      <td>-1.528</td>\n",
       "      <td>-3.520</td>\n",
       "      <td>2.055</td>\n",
       "      <td>-0.234</td>\n",
       "      <td>-0.358</td>\n",
       "      <td>-3.782</td>\n",
       "      <td>2.180</td>\n",
       "      <td>6.112</td>\n",
       "      <td>1.985</td>\n",
       "      <td>-8.330</td>\n",
       "      <td>-1.639</td>\n",
       "      <td>-0.915</td>\n",
       "      <td>5.672</td>\n",
       "      <td>-3.924</td>\n",
       "      <td>2.133</td>\n",
       "      <td>-4.502</td>\n",
       "      <td>2.777</td>\n",
       "      <td>5.728</td>\n",
       "      <td>1.620</td>\n",
       "      <td>-1.700</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>-2.923</td>\n",
       "      <td>-2.760</td>\n",
       "      <td>-2.254</td>\n",
       "      <td>2.552</td>\n",
       "      <td>0.982</td>\n",
       "      <td>7.112</td>\n",
       "      <td>1.476</td>\n",
       "      <td>-3.954</td>\n",
       "      <td>1.856</td>\n",
       "      <td>5.029</td>\n",
       "      <td>2.083</td>\n",
       "      <td>-6.409</td>\n",
       "      <td>1.477</td>\n",
       "      <td>-0.874</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39996</th>\n",
       "      <td>-3.187</td>\n",
       "      <td>-10.052</td>\n",
       "      <td>5.696</td>\n",
       "      <td>-4.370</td>\n",
       "      <td>-5.355</td>\n",
       "      <td>-1.873</td>\n",
       "      <td>-3.947</td>\n",
       "      <td>0.679</td>\n",
       "      <td>-2.389</td>\n",
       "      <td>5.457</td>\n",
       "      <td>1.583</td>\n",
       "      <td>3.571</td>\n",
       "      <td>9.227</td>\n",
       "      <td>2.554</td>\n",
       "      <td>-7.039</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>-9.665</td>\n",
       "      <td>1.155</td>\n",
       "      <td>3.877</td>\n",
       "      <td>3.524</td>\n",
       "      <td>-7.015</td>\n",
       "      <td>-0.132</td>\n",
       "      <td>-3.446</td>\n",
       "      <td>-4.801</td>\n",
       "      <td>-0.876</td>\n",
       "      <td>-3.812</td>\n",
       "      <td>5.422</td>\n",
       "      <td>-3.732</td>\n",
       "      <td>0.609</td>\n",
       "      <td>5.256</td>\n",
       "      <td>1.915</td>\n",
       "      <td>0.403</td>\n",
       "      <td>3.164</td>\n",
       "      <td>3.752</td>\n",
       "      <td>8.530</td>\n",
       "      <td>8.451</td>\n",
       "      <td>0.204</td>\n",
       "      <td>-7.130</td>\n",
       "      <td>4.249</td>\n",
       "      <td>-6.112</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39997</th>\n",
       "      <td>-2.687</td>\n",
       "      <td>1.961</td>\n",
       "      <td>6.137</td>\n",
       "      <td>2.600</td>\n",
       "      <td>2.657</td>\n",
       "      <td>-4.291</td>\n",
       "      <td>-2.344</td>\n",
       "      <td>0.974</td>\n",
       "      <td>-1.027</td>\n",
       "      <td>0.497</td>\n",
       "      <td>-9.589</td>\n",
       "      <td>3.177</td>\n",
       "      <td>1.055</td>\n",
       "      <td>-1.416</td>\n",
       "      <td>-4.669</td>\n",
       "      <td>-5.405</td>\n",
       "      <td>3.720</td>\n",
       "      <td>2.893</td>\n",
       "      <td>2.329</td>\n",
       "      <td>1.458</td>\n",
       "      <td>-6.429</td>\n",
       "      <td>1.818</td>\n",
       "      <td>0.806</td>\n",
       "      <td>7.786</td>\n",
       "      <td>0.331</td>\n",
       "      <td>5.257</td>\n",
       "      <td>-4.867</td>\n",
       "      <td>-0.819</td>\n",
       "      <td>-5.667</td>\n",
       "      <td>-2.861</td>\n",
       "      <td>4.674</td>\n",
       "      <td>6.621</td>\n",
       "      <td>-1.989</td>\n",
       "      <td>-1.349</td>\n",
       "      <td>3.952</td>\n",
       "      <td>5.450</td>\n",
       "      <td>-0.455</td>\n",
       "      <td>-2.202</td>\n",
       "      <td>1.678</td>\n",
       "      <td>-1.974</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39998</th>\n",
       "      <td>0.521</td>\n",
       "      <td>0.096</td>\n",
       "      <td>8.457</td>\n",
       "      <td>2.138</td>\n",
       "      <td>-1.636</td>\n",
       "      <td>-2.713</td>\n",
       "      <td>-2.693</td>\n",
       "      <td>-3.410</td>\n",
       "      <td>1.936</td>\n",
       "      <td>2.012</td>\n",
       "      <td>-4.989</td>\n",
       "      <td>-0.819</td>\n",
       "      <td>4.166</td>\n",
       "      <td>-1.192</td>\n",
       "      <td>-5.033</td>\n",
       "      <td>-8.523</td>\n",
       "      <td>-1.950</td>\n",
       "      <td>0.017</td>\n",
       "      <td>4.505</td>\n",
       "      <td>2.031</td>\n",
       "      <td>-8.849</td>\n",
       "      <td>0.566</td>\n",
       "      <td>-6.040</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>1.656</td>\n",
       "      <td>4.250</td>\n",
       "      <td>1.727</td>\n",
       "      <td>-1.686</td>\n",
       "      <td>-3.963</td>\n",
       "      <td>-2.642</td>\n",
       "      <td>1.939</td>\n",
       "      <td>-1.257</td>\n",
       "      <td>-1.136</td>\n",
       "      <td>1.434</td>\n",
       "      <td>5.905</td>\n",
       "      <td>3.752</td>\n",
       "      <td>-1.867</td>\n",
       "      <td>-1.918</td>\n",
       "      <td>2.573</td>\n",
       "      <td>-5.019</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39999</th>\n",
       "      <td>2.403</td>\n",
       "      <td>-1.336</td>\n",
       "      <td>6.451</td>\n",
       "      <td>-5.356</td>\n",
       "      <td>-0.434</td>\n",
       "      <td>0.255</td>\n",
       "      <td>-1.120</td>\n",
       "      <td>-2.523</td>\n",
       "      <td>-0.654</td>\n",
       "      <td>2.316</td>\n",
       "      <td>-2.862</td>\n",
       "      <td>0.199</td>\n",
       "      <td>1.593</td>\n",
       "      <td>-0.337</td>\n",
       "      <td>-0.709</td>\n",
       "      <td>-4.408</td>\n",
       "      <td>-3.683</td>\n",
       "      <td>2.973</td>\n",
       "      <td>-1.223</td>\n",
       "      <td>-1.958</td>\n",
       "      <td>-4.454</td>\n",
       "      <td>0.464</td>\n",
       "      <td>-4.952</td>\n",
       "      <td>-1.624</td>\n",
       "      <td>2.965</td>\n",
       "      <td>2.009</td>\n",
       "      <td>5.712</td>\n",
       "      <td>-2.910</td>\n",
       "      <td>-2.287</td>\n",
       "      <td>-3.676</td>\n",
       "      <td>5.678</td>\n",
       "      <td>-4.310</td>\n",
       "      <td>-0.709</td>\n",
       "      <td>-1.359</td>\n",
       "      <td>1.639</td>\n",
       "      <td>7.766</td>\n",
       "      <td>-0.245</td>\n",
       "      <td>-1.124</td>\n",
       "      <td>2.872</td>\n",
       "      <td>1.902</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          V1      V2     V3     V4     V5     V6     V7     V8     V9    V10  \\\n",
       "39990  1.850   3.606  0.979  3.262  0.353 -0.267 -0.522 -3.015  1.819 -0.391   \n",
       "39991 -2.688  -2.626 -2.364 -3.041 -0.649  1.083 -0.171 -0.444 -1.568  0.773   \n",
       "39992  8.061   7.361  4.239  6.525 -1.838 -1.866 -0.113 -4.718  6.287 -3.393   \n",
       "39993  2.890   2.483  5.644  0.937 -1.381  0.412 -1.593 -5.762  2.150  0.272   \n",
       "39994  2.274   4.197  2.346  1.343  1.889 -1.749  1.382 -0.209  1.099 -0.876   \n",
       "39995 -3.897  -3.942 -0.351 -2.417  1.108 -1.528 -3.520  2.055 -0.234 -0.358   \n",
       "39996 -3.187 -10.052  5.696 -4.370 -5.355 -1.873 -3.947  0.679 -2.389  5.457   \n",
       "39997 -2.687   1.961  6.137  2.600  2.657 -4.291 -2.344  0.974 -1.027  0.497   \n",
       "39998  0.521   0.096  8.457  2.138 -1.636 -2.713 -2.693 -3.410  1.936  2.012   \n",
       "39999  2.403  -1.336  6.451 -5.356 -0.434  0.255 -1.120 -2.523 -0.654  2.316   \n",
       "\n",
       "         V11    V12    V13    V14    V15     V16    V17    V18    V19    V20  \\\n",
       "39990 -1.404 -1.286  0.050 -2.083 -2.609  -6.315  0.591  0.077  2.901 -2.107   \n",
       "39991  1.260  3.761  1.152  0.048 -1.854  -0.476 -1.188  3.130  0.222  0.149   \n",
       "39992 -3.122 -2.927  0.722 -3.996 -2.406 -12.387  3.764 -2.313  5.224  0.209   \n",
       "39993 -2.095 -1.526  0.072 -3.540 -2.762 -10.632 -0.495  1.720  3.872 -1.210   \n",
       "39994 -4.681  0.347 -1.105 -0.692  1.752  -1.806  3.145 -0.838 -0.199 -0.510   \n",
       "39995 -3.782  2.180  6.112  1.985 -8.330  -1.639 -0.915  5.672 -3.924  2.133   \n",
       "39996  1.583  3.571  9.227  2.554 -7.039  -0.994 -9.665  1.155  3.877  3.524   \n",
       "39997 -9.589  3.177  1.055 -1.416 -4.669  -5.405  3.720  2.893  2.329  1.458   \n",
       "39998 -4.989 -0.819  4.166 -1.192 -5.033  -8.523 -1.950  0.017  4.505  2.031   \n",
       "39999 -2.862  0.199  1.593 -0.337 -0.709  -4.408 -3.683  2.973 -1.223 -1.958   \n",
       "\n",
       "          V21    V22    V23    V24    V25    V26    V27    V28    V29    V30  \\\n",
       "39990  -5.172  2.504  0.272  4.264 -0.816  3.828 -1.985  0.466 -1.774 -0.034   \n",
       "39991   0.107  1.543  2.409 -0.166 -1.128 -3.146  0.881  0.472  0.623  2.001   \n",
       "39992 -10.672  3.958 -2.357  1.370  2.184  6.374 -1.096  0.573 -3.013 -2.412   \n",
       "39993  -8.222  2.121 -5.492  1.452  1.450  3.685  1.077 -0.384 -0.839 -0.748   \n",
       "39994  -0.524  0.312 -0.351  1.324  2.252  3.851  0.482  0.200 -4.472 -6.366   \n",
       "39995  -4.502  2.777  5.728  1.620 -1.700 -0.042 -2.923 -2.760 -2.254  2.552   \n",
       "39996  -7.015 -0.132 -3.446 -4.801 -0.876 -3.812  5.422 -3.732  0.609  5.256   \n",
       "39997  -6.429  1.818  0.806  7.786  0.331  5.257 -4.867 -0.819 -5.667 -2.861   \n",
       "39998  -8.849  0.566 -6.040 -0.043  1.656  4.250  1.727 -1.686 -3.963 -2.642   \n",
       "39999  -4.454  0.464 -4.952 -1.624  2.965  2.009  5.712 -2.910 -2.287 -3.676   \n",
       "\n",
       "         V31    V32    V33    V34    V35    V36    V37    V38    V39    V40  \\\n",
       "39990 -1.432  2.894  1.355 -1.563  3.778 -3.700 -2.162  3.273 -1.031 -2.999   \n",
       "39991 -3.543 -1.826  0.128  1.531 -0.967  1.861  2.256  2.277  1.990 -0.152   \n",
       "39992 -4.315 -3.117 -2.818 -1.810  5.792 -8.113 -2.931  2.015 -2.773 -3.375   \n",
       "39993 -1.089 -4.159  1.181 -0.742  5.369 -0.693 -1.669  3.660  0.820 -1.987   \n",
       "39994  2.263 -1.375 -5.227 -0.186 -2.636  1.344 -0.381  0.589  0.340  1.232   \n",
       "39995  0.982  7.112  1.476 -3.954  1.856  5.029  2.083 -6.409  1.477 -0.874   \n",
       "39996  1.915  0.403  3.164  3.752  8.530  8.451  0.204 -7.130  4.249 -6.112   \n",
       "39997  4.674  6.621 -1.989 -1.349  3.952  5.450 -0.455 -2.202  1.678 -1.974   \n",
       "39998  1.939 -1.257 -1.136  1.434  5.905  3.752 -1.867 -1.918  2.573 -5.019   \n",
       "39999  5.678 -4.310 -0.709 -1.359  1.639  7.766 -0.245 -1.124  2.872  1.902   \n",
       "\n",
       "       Target  \n",
       "39990       0  \n",
       "39991       0  \n",
       "39992       0  \n",
       "39993       0  \n",
       "39994       0  \n",
       "39995       0  \n",
       "39996       0  \n",
       "39997       0  \n",
       "39998       0  \n",
       "39999       0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's view the last 10 rows of the data\n",
    "data.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 40000 entries, 0 to 39999\n",
      "Data columns (total 41 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   V1      39954 non-null  float64\n",
      " 1   V2      39961 non-null  float64\n",
      " 2   V3      40000 non-null  float64\n",
      " 3   V4      40000 non-null  float64\n",
      " 4   V5      40000 non-null  float64\n",
      " 5   V6      40000 non-null  float64\n",
      " 6   V7      40000 non-null  float64\n",
      " 7   V8      40000 non-null  float64\n",
      " 8   V9      40000 non-null  float64\n",
      " 9   V10     40000 non-null  float64\n",
      " 10  V11     40000 non-null  float64\n",
      " 11  V12     40000 non-null  float64\n",
      " 12  V13     40000 non-null  float64\n",
      " 13  V14     40000 non-null  float64\n",
      " 14  V15     40000 non-null  float64\n",
      " 15  V16     40000 non-null  float64\n",
      " 16  V17     40000 non-null  float64\n",
      " 17  V18     40000 non-null  float64\n",
      " 18  V19     40000 non-null  float64\n",
      " 19  V20     40000 non-null  float64\n",
      " 20  V21     40000 non-null  float64\n",
      " 21  V22     40000 non-null  float64\n",
      " 22  V23     40000 non-null  float64\n",
      " 23  V24     40000 non-null  float64\n",
      " 24  V25     40000 non-null  float64\n",
      " 25  V26     40000 non-null  float64\n",
      " 26  V27     40000 non-null  float64\n",
      " 27  V28     40000 non-null  float64\n",
      " 28  V29     40000 non-null  float64\n",
      " 29  V30     40000 non-null  float64\n",
      " 30  V31     40000 non-null  float64\n",
      " 31  V32     40000 non-null  float64\n",
      " 32  V33     40000 non-null  float64\n",
      " 33  V34     40000 non-null  float64\n",
      " 34  V35     40000 non-null  float64\n",
      " 35  V36     40000 non-null  float64\n",
      " 36  V37     40000 non-null  float64\n",
      " 37  V38     40000 non-null  float64\n",
      " 38  V39     40000 non-null  float64\n",
      " 39  V40     40000 non-null  float64\n",
      " 40  Target  40000 non-null  int64  \n",
      "dtypes: float64(40), int64(1)\n",
      "memory usage: 12.5 MB\n"
     ]
    }
   ],
   "source": [
    "# let's check the data types of the columns in the dataset\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a total of 41 columns and 40,000 observations in the dataset.\n",
    "We can see that there are no null values i.e. column have missing values. \n",
    "The Target column is int64 type while the other 40 columns are of type float64."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's check for duplicate values in the data\n",
    "data.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data set does not contain duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "V1       0.120\n",
       "V2       0.100\n",
       "V3       0.000\n",
       "V4       0.000\n",
       "V5       0.000\n",
       "V6       0.000\n",
       "V7       0.000\n",
       "V8       0.000\n",
       "V9       0.000\n",
       "V10      0.000\n",
       "V11      0.000\n",
       "V12      0.000\n",
       "V13      0.000\n",
       "V14      0.000\n",
       "V15      0.000\n",
       "V16      0.000\n",
       "V17      0.000\n",
       "V18      0.000\n",
       "V19      0.000\n",
       "V20      0.000\n",
       "V21      0.000\n",
       "V22      0.000\n",
       "V23      0.000\n",
       "V24      0.000\n",
       "V25      0.000\n",
       "V26      0.000\n",
       "V27      0.000\n",
       "V28      0.000\n",
       "V29      0.000\n",
       "V30      0.000\n",
       "V31      0.000\n",
       "V32      0.000\n",
       "V33      0.000\n",
       "V34      0.000\n",
       "V35      0.000\n",
       "V36      0.000\n",
       "V37      0.000\n",
       "V38      0.000\n",
       "V39      0.000\n",
       "V40      0.000\n",
       "Target   0.000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's check for missing values in the data\n",
    "round(data.isnull().sum() / data.isnull().count() * 100, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Column V1 has 0.12% missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>V1</th>\n",
       "      <td>39954.000</td>\n",
       "      <td>-0.288</td>\n",
       "      <td>3.449</td>\n",
       "      <td>-13.502</td>\n",
       "      <td>-2.751</td>\n",
       "      <td>-0.774</td>\n",
       "      <td>1.837</td>\n",
       "      <td>17.437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V2</th>\n",
       "      <td>39961.000</td>\n",
       "      <td>0.443</td>\n",
       "      <td>3.139</td>\n",
       "      <td>-13.212</td>\n",
       "      <td>-1.638</td>\n",
       "      <td>0.464</td>\n",
       "      <td>2.538</td>\n",
       "      <td>13.089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V3</th>\n",
       "      <td>40000.000</td>\n",
       "      <td>2.506</td>\n",
       "      <td>3.406</td>\n",
       "      <td>-11.469</td>\n",
       "      <td>0.203</td>\n",
       "      <td>2.265</td>\n",
       "      <td>4.585</td>\n",
       "      <td>18.366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V4</th>\n",
       "      <td>40000.000</td>\n",
       "      <td>-0.066</td>\n",
       "      <td>3.437</td>\n",
       "      <td>-16.015</td>\n",
       "      <td>-2.350</td>\n",
       "      <td>-0.124</td>\n",
       "      <td>2.149</td>\n",
       "      <td>13.280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V5</th>\n",
       "      <td>40000.000</td>\n",
       "      <td>-0.045</td>\n",
       "      <td>2.107</td>\n",
       "      <td>-8.613</td>\n",
       "      <td>-1.507</td>\n",
       "      <td>-0.097</td>\n",
       "      <td>1.346</td>\n",
       "      <td>9.403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V6</th>\n",
       "      <td>40000.000</td>\n",
       "      <td>-1.001</td>\n",
       "      <td>2.037</td>\n",
       "      <td>-10.227</td>\n",
       "      <td>-2.363</td>\n",
       "      <td>-1.007</td>\n",
       "      <td>0.374</td>\n",
       "      <td>7.065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V7</th>\n",
       "      <td>40000.000</td>\n",
       "      <td>-0.893</td>\n",
       "      <td>1.757</td>\n",
       "      <td>-8.206</td>\n",
       "      <td>-2.037</td>\n",
       "      <td>-0.935</td>\n",
       "      <td>0.207</td>\n",
       "      <td>8.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V8</th>\n",
       "      <td>40000.000</td>\n",
       "      <td>-0.563</td>\n",
       "      <td>3.299</td>\n",
       "      <td>-15.658</td>\n",
       "      <td>-2.660</td>\n",
       "      <td>-0.384</td>\n",
       "      <td>1.714</td>\n",
       "      <td>11.679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V9</th>\n",
       "      <td>40000.000</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>2.162</td>\n",
       "      <td>-8.596</td>\n",
       "      <td>-1.494</td>\n",
       "      <td>-0.052</td>\n",
       "      <td>1.426</td>\n",
       "      <td>8.507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V10</th>\n",
       "      <td>40000.000</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>2.183</td>\n",
       "      <td>-11.001</td>\n",
       "      <td>-1.391</td>\n",
       "      <td>0.106</td>\n",
       "      <td>1.486</td>\n",
       "      <td>8.108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V11</th>\n",
       "      <td>40000.000</td>\n",
       "      <td>-1.918</td>\n",
       "      <td>3.116</td>\n",
       "      <td>-14.832</td>\n",
       "      <td>-3.941</td>\n",
       "      <td>-1.942</td>\n",
       "      <td>0.089</td>\n",
       "      <td>13.852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V12</th>\n",
       "      <td>40000.000</td>\n",
       "      <td>1.578</td>\n",
       "      <td>2.915</td>\n",
       "      <td>-13.619</td>\n",
       "      <td>-0.431</td>\n",
       "      <td>1.485</td>\n",
       "      <td>3.541</td>\n",
       "      <td>15.754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V13</th>\n",
       "      <td>40000.000</td>\n",
       "      <td>1.591</td>\n",
       "      <td>2.865</td>\n",
       "      <td>-13.830</td>\n",
       "      <td>-0.209</td>\n",
       "      <td>1.654</td>\n",
       "      <td>3.476</td>\n",
       "      <td>15.420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V14</th>\n",
       "      <td>40000.000</td>\n",
       "      <td>-0.947</td>\n",
       "      <td>1.788</td>\n",
       "      <td>-8.309</td>\n",
       "      <td>-2.165</td>\n",
       "      <td>-0.957</td>\n",
       "      <td>0.266</td>\n",
       "      <td>6.213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V15</th>\n",
       "      <td>40000.000</td>\n",
       "      <td>-2.436</td>\n",
       "      <td>3.341</td>\n",
       "      <td>-17.202</td>\n",
       "      <td>-4.451</td>\n",
       "      <td>-2.399</td>\n",
       "      <td>-0.382</td>\n",
       "      <td>12.875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V16</th>\n",
       "      <td>40000.000</td>\n",
       "      <td>-2.943</td>\n",
       "      <td>4.212</td>\n",
       "      <td>-21.919</td>\n",
       "      <td>-5.632</td>\n",
       "      <td>-2.719</td>\n",
       "      <td>-0.113</td>\n",
       "      <td>13.583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V17</th>\n",
       "      <td>40000.000</td>\n",
       "      <td>-0.143</td>\n",
       "      <td>3.344</td>\n",
       "      <td>-17.634</td>\n",
       "      <td>-2.227</td>\n",
       "      <td>-0.028</td>\n",
       "      <td>2.072</td>\n",
       "      <td>17.405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V18</th>\n",
       "      <td>40000.000</td>\n",
       "      <td>1.189</td>\n",
       "      <td>2.586</td>\n",
       "      <td>-11.644</td>\n",
       "      <td>-0.403</td>\n",
       "      <td>0.867</td>\n",
       "      <td>2.564</td>\n",
       "      <td>13.180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V19</th>\n",
       "      <td>40000.000</td>\n",
       "      <td>1.181</td>\n",
       "      <td>3.395</td>\n",
       "      <td>-13.492</td>\n",
       "      <td>-1.051</td>\n",
       "      <td>1.278</td>\n",
       "      <td>3.497</td>\n",
       "      <td>16.059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V20</th>\n",
       "      <td>40000.000</td>\n",
       "      <td>0.027</td>\n",
       "      <td>3.675</td>\n",
       "      <td>-13.923</td>\n",
       "      <td>-2.434</td>\n",
       "      <td>0.030</td>\n",
       "      <td>2.513</td>\n",
       "      <td>16.052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V21</th>\n",
       "      <td>40000.000</td>\n",
       "      <td>-3.621</td>\n",
       "      <td>3.557</td>\n",
       "      <td>-19.436</td>\n",
       "      <td>-5.921</td>\n",
       "      <td>-3.559</td>\n",
       "      <td>-1.284</td>\n",
       "      <td>13.840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V22</th>\n",
       "      <td>40000.000</td>\n",
       "      <td>0.943</td>\n",
       "      <td>1.646</td>\n",
       "      <td>-10.122</td>\n",
       "      <td>-0.112</td>\n",
       "      <td>0.963</td>\n",
       "      <td>2.018</td>\n",
       "      <td>7.410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V23</th>\n",
       "      <td>40000.000</td>\n",
       "      <td>-0.388</td>\n",
       "      <td>4.052</td>\n",
       "      <td>-16.188</td>\n",
       "      <td>-3.119</td>\n",
       "      <td>-0.275</td>\n",
       "      <td>2.438</td>\n",
       "      <td>15.080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V24</th>\n",
       "      <td>40000.000</td>\n",
       "      <td>1.142</td>\n",
       "      <td>3.913</td>\n",
       "      <td>-18.488</td>\n",
       "      <td>-1.483</td>\n",
       "      <td>0.964</td>\n",
       "      <td>3.563</td>\n",
       "      <td>19.769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V25</th>\n",
       "      <td>40000.000</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>2.025</td>\n",
       "      <td>-8.228</td>\n",
       "      <td>-1.373</td>\n",
       "      <td>0.021</td>\n",
       "      <td>1.400</td>\n",
       "      <td>8.223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V26</th>\n",
       "      <td>40000.000</td>\n",
       "      <td>1.896</td>\n",
       "      <td>3.421</td>\n",
       "      <td>-12.588</td>\n",
       "      <td>-0.319</td>\n",
       "      <td>1.964</td>\n",
       "      <td>4.163</td>\n",
       "      <td>16.836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V27</th>\n",
       "      <td>40000.000</td>\n",
       "      <td>-0.617</td>\n",
       "      <td>4.392</td>\n",
       "      <td>-14.905</td>\n",
       "      <td>-3.692</td>\n",
       "      <td>-0.910</td>\n",
       "      <td>2.201</td>\n",
       "      <td>21.595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V28</th>\n",
       "      <td>40000.000</td>\n",
       "      <td>-0.888</td>\n",
       "      <td>1.925</td>\n",
       "      <td>-9.685</td>\n",
       "      <td>-2.193</td>\n",
       "      <td>-0.905</td>\n",
       "      <td>0.377</td>\n",
       "      <td>6.907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V29</th>\n",
       "      <td>40000.000</td>\n",
       "      <td>-1.005</td>\n",
       "      <td>2.676</td>\n",
       "      <td>-12.579</td>\n",
       "      <td>-2.799</td>\n",
       "      <td>-1.206</td>\n",
       "      <td>0.604</td>\n",
       "      <td>11.852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V30</th>\n",
       "      <td>40000.000</td>\n",
       "      <td>-0.033</td>\n",
       "      <td>3.031</td>\n",
       "      <td>-14.796</td>\n",
       "      <td>-1.908</td>\n",
       "      <td>0.185</td>\n",
       "      <td>2.040</td>\n",
       "      <td>13.191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V31</th>\n",
       "      <td>40000.000</td>\n",
       "      <td>0.506</td>\n",
       "      <td>3.483</td>\n",
       "      <td>-19.377</td>\n",
       "      <td>-1.799</td>\n",
       "      <td>0.491</td>\n",
       "      <td>2.778</td>\n",
       "      <td>17.255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V32</th>\n",
       "      <td>40000.000</td>\n",
       "      <td>0.327</td>\n",
       "      <td>5.499</td>\n",
       "      <td>-23.201</td>\n",
       "      <td>-3.392</td>\n",
       "      <td>0.056</td>\n",
       "      <td>3.789</td>\n",
       "      <td>24.848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V33</th>\n",
       "      <td>40000.000</td>\n",
       "      <td>0.057</td>\n",
       "      <td>3.574</td>\n",
       "      <td>-17.454</td>\n",
       "      <td>-2.238</td>\n",
       "      <td>-0.050</td>\n",
       "      <td>2.256</td>\n",
       "      <td>16.692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V34</th>\n",
       "      <td>40000.000</td>\n",
       "      <td>-0.464</td>\n",
       "      <td>3.186</td>\n",
       "      <td>-17.985</td>\n",
       "      <td>-2.128</td>\n",
       "      <td>-0.251</td>\n",
       "      <td>1.433</td>\n",
       "      <td>14.358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V35</th>\n",
       "      <td>40000.000</td>\n",
       "      <td>2.235</td>\n",
       "      <td>2.924</td>\n",
       "      <td>-15.350</td>\n",
       "      <td>0.332</td>\n",
       "      <td>2.110</td>\n",
       "      <td>4.045</td>\n",
       "      <td>16.805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V36</th>\n",
       "      <td>40000.000</td>\n",
       "      <td>1.530</td>\n",
       "      <td>3.820</td>\n",
       "      <td>-17.479</td>\n",
       "      <td>-0.937</td>\n",
       "      <td>1.572</td>\n",
       "      <td>3.997</td>\n",
       "      <td>19.330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V37</th>\n",
       "      <td>40000.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>1.778</td>\n",
       "      <td>-7.640</td>\n",
       "      <td>-1.266</td>\n",
       "      <td>-0.133</td>\n",
       "      <td>1.161</td>\n",
       "      <td>7.803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V38</th>\n",
       "      <td>40000.000</td>\n",
       "      <td>-0.351</td>\n",
       "      <td>3.964</td>\n",
       "      <td>-17.375</td>\n",
       "      <td>-3.017</td>\n",
       "      <td>-0.319</td>\n",
       "      <td>2.291</td>\n",
       "      <td>15.964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V39</th>\n",
       "      <td>40000.000</td>\n",
       "      <td>0.900</td>\n",
       "      <td>1.751</td>\n",
       "      <td>-7.136</td>\n",
       "      <td>-0.262</td>\n",
       "      <td>0.921</td>\n",
       "      <td>2.069</td>\n",
       "      <td>7.998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V40</th>\n",
       "      <td>40000.000</td>\n",
       "      <td>-0.897</td>\n",
       "      <td>2.998</td>\n",
       "      <td>-11.930</td>\n",
       "      <td>-2.950</td>\n",
       "      <td>-0.949</td>\n",
       "      <td>1.092</td>\n",
       "      <td>10.654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Target</th>\n",
       "      <td>40000.000</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.227</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           count   mean   std     min    25%    50%    75%    max\n",
       "V1     39954.000 -0.288 3.449 -13.502 -2.751 -0.774  1.837 17.437\n",
       "V2     39961.000  0.443 3.139 -13.212 -1.638  0.464  2.538 13.089\n",
       "V3     40000.000  2.506 3.406 -11.469  0.203  2.265  4.585 18.366\n",
       "V4     40000.000 -0.066 3.437 -16.015 -2.350 -0.124  2.149 13.280\n",
       "V5     40000.000 -0.045 2.107  -8.613 -1.507 -0.097  1.346  9.403\n",
       "V6     40000.000 -1.001 2.037 -10.227 -2.363 -1.007  0.374  7.065\n",
       "V7     40000.000 -0.893 1.757  -8.206 -2.037 -0.935  0.207  8.006\n",
       "V8     40000.000 -0.563 3.299 -15.658 -2.660 -0.384  1.714 11.679\n",
       "V9     40000.000 -0.008 2.162  -8.596 -1.494 -0.052  1.426  8.507\n",
       "V10    40000.000 -0.002 2.183 -11.001 -1.391  0.106  1.486  8.108\n",
       "V11    40000.000 -1.918 3.116 -14.832 -3.941 -1.942  0.089 13.852\n",
       "V12    40000.000  1.578 2.915 -13.619 -0.431  1.485  3.541 15.754\n",
       "V13    40000.000  1.591 2.865 -13.830 -0.209  1.654  3.476 15.420\n",
       "V14    40000.000 -0.947 1.788  -8.309 -2.165 -0.957  0.266  6.213\n",
       "V15    40000.000 -2.436 3.341 -17.202 -4.451 -2.399 -0.382 12.875\n",
       "V16    40000.000 -2.943 4.212 -21.919 -5.632 -2.719 -0.113 13.583\n",
       "V17    40000.000 -0.143 3.344 -17.634 -2.227 -0.028  2.072 17.405\n",
       "V18    40000.000  1.189 2.586 -11.644 -0.403  0.867  2.564 13.180\n",
       "V19    40000.000  1.181 3.395 -13.492 -1.051  1.278  3.497 16.059\n",
       "V20    40000.000  0.027 3.675 -13.923 -2.434  0.030  2.513 16.052\n",
       "V21    40000.000 -3.621 3.557 -19.436 -5.921 -3.559 -1.284 13.840\n",
       "V22    40000.000  0.943 1.646 -10.122 -0.112  0.963  2.018  7.410\n",
       "V23    40000.000 -0.388 4.052 -16.188 -3.119 -0.275  2.438 15.080\n",
       "V24    40000.000  1.142 3.913 -18.488 -1.483  0.964  3.563 19.769\n",
       "V25    40000.000 -0.003 2.025  -8.228 -1.373  0.021  1.400  8.223\n",
       "V26    40000.000  1.896 3.421 -12.588 -0.319  1.964  4.163 16.836\n",
       "V27    40000.000 -0.617 4.392 -14.905 -3.692 -0.910  2.201 21.595\n",
       "V28    40000.000 -0.888 1.925  -9.685 -2.193 -0.905  0.377  6.907\n",
       "V29    40000.000 -1.005 2.676 -12.579 -2.799 -1.206  0.604 11.852\n",
       "V30    40000.000 -0.033 3.031 -14.796 -1.908  0.185  2.040 13.191\n",
       "V31    40000.000  0.506 3.483 -19.377 -1.799  0.491  2.778 17.255\n",
       "V32    40000.000  0.327 5.499 -23.201 -3.392  0.056  3.789 24.848\n",
       "V33    40000.000  0.057 3.574 -17.454 -2.238 -0.050  2.256 16.692\n",
       "V34    40000.000 -0.464 3.186 -17.985 -2.128 -0.251  1.433 14.358\n",
       "V35    40000.000  2.235 2.924 -15.350  0.332  2.110  4.045 16.805\n",
       "V36    40000.000  1.530 3.820 -17.479 -0.937  1.572  3.997 19.330\n",
       "V37    40000.000 -0.000 1.778  -7.640 -1.266 -0.133  1.161  7.803\n",
       "V38    40000.000 -0.351 3.964 -17.375 -3.017 -0.319  2.291 15.964\n",
       "V39    40000.000  0.900 1.751  -7.136 -0.262  0.921  2.069  7.998\n",
       "V40    40000.000 -0.897 2.998 -11.930 -2.950 -0.949  1.092 10.654\n",
       "Target 40000.000  0.055 0.227   0.000  0.000  0.000  0.000  1.000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's view the statistical summary of the numerical columns in the data\n",
    "data.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "knk0w9XH4jao"
   },
   "source": [
    "## Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "RW = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "2JbJc1bX4jao"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-f2043371e284>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.25\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m )\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'X_test' is not defined"
     ]
    }
   ],
   "source": [
    "X = RW.X = RW.drop([\"V1\"], axis=1)\n",
    "y = RW[\"V1\"]\n",
    "\n",
    "# then we split the temporary set into train and validation\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=1,\n",
    ")\n",
    "print(X_train.shape, X_val.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ONL1sM1n4jap"
   },
   "source": [
    "## Model evaluation criterion\n",
    "\n",
    "### 3 types of cost are associated with the provided problem\n",
    "1. Replacement cost - False Negatives - Predicting no failure, while there will be a failure\n",
    "2. Inspection cost - False Positives - Predicting failure, while there is no failure \n",
    "3. Repair cost - True Positives - Predicting failure correctly\n",
    "\n",
    "### How to reduce the overall cost?\n",
    "* We need to create a customized metric, that can help to bring down the overall cost.\n",
    "* The cost associated with any model = TP * 15000 + FP * 5000 + FN * 40000\n",
    "* And the minimum possible cost will be when, the model will be able to identify all failures, in that case, the cost will be (TP + FN) * 15000\n",
    "* So, we will try to maximize `Minimum cost/Cost associated with model`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "djQTqGKU4jap"
   },
   "source": [
    "**Let's create two functions to calculate different metrics and confusion matrix, so that we don't have to use the same code repeatedly for each model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "bIekBxwp4jaq"
   },
   "outputs": [],
   "source": [
    "# defining a function to compute different metrics to check performance of a classification model built using sklearn\n",
    "def model_performance_classification_sklearn(model, predictors, target):\n",
    "    \"\"\"\n",
    "    Function to compute different metrics to check classification model performance\n",
    "\n",
    "    model: classifier\n",
    "    predictors: independent variables\n",
    "    target: dependent variable\n",
    "    \"\"\"\n",
    "\n",
    "    TP = confusion_matrix(target, model.predict(predictors))[1, 1]\n",
    "    FP = confusion_matrix(target, model.predict(predictors))[0, 1]\n",
    "    FN = confusion_matrix(target, model.predict(predictors))[1, 0]\n",
    "    Cost = TP * 15 + FP * 5 + FN * 40  # maintenance cost by using model\n",
    "    Min_Cost = (TP + FN) * 15  # minimum possible maintenance cost = number of actual positives\n",
    "    Percent = Min_Cost / Cost  # ratio of minimum possible maintenance cost and maintenance cost by model\n",
    "\n",
    "    # predicting using the independent variables\n",
    "    pred = model.predict(predictors)\n",
    "\n",
    "    acc = accuracy_score(target, pred)  # to compute Accuracy\n",
    "    recall = recall_score(target, pred)  # to compute Recall\n",
    "    precision = precision_score(target, pred)  # to compute Precision\n",
    "    f1 = f1_score(target, pred)  # to compute F1-score\n",
    "\n",
    "    # creating a dataframe of metrics\n",
    "    df_perf = pd.DataFrame(\n",
    "        {\n",
    "            \"Accuracy\": acc,\n",
    "            \"Recall\": recall,\n",
    "            \"Precision\": precision,\n",
    "            \"F1\": f1,\n",
    "            \"Minimum_Vs_Model_cost\": Percent,\n",
    "        },\n",
    "        index=[0],\n",
    "    )\n",
    "\n",
    "    return df_perf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "8LXyI50s4jar"
   },
   "outputs": [],
   "source": [
    "def confusion_matrix_sklearn(model, predictors, target):\n",
    "    \"\"\"\n",
    "    To plot the confusion_matrix with percentages\n",
    "\n",
    "    model: classifier\n",
    "    predictors: independent variables\n",
    "    target: dependent variable\n",
    "    \"\"\"\n",
    "    y_pred = model.predict(predictors)\n",
    "    cm = confusion_matrix(target, y_pred)\n",
    "    labels = np.asarray(\n",
    "        [\n",
    "            [\"{0:0.0f}\".format(item) + \"\\n{0:.2%}\".format(item / cm.flatten().sum())]\n",
    "            for item in cm.flatten()\n",
    "        ]\n",
    "    ).reshape(2, 2)\n",
    "\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.heatmap(cm, annot=labels, fmt=\"\")\n",
    "    plt.ylabel(\"True label\")\n",
    "    plt.xlabel(\"Predicted label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rxw_gopM4jar"
   },
   "source": [
    "## **Defining scorer to be used for hyperparameter tuning**\n",
    "\n",
    "- Every prediction of a classification model will be either a TP, FP, FN or TN.\n",
    "- Till now at most of the places we wanted to reduce False negatives and hence we tried to maximize \"Recall\".\n",
    "- To maximize Recall, we used Recall as a **scorer** in hyperparameter tuning.\n",
    "- Here, we not only want to minimize false negatives but also false positives and we are also given the costs associated with each type of prediction.\n",
    "- So, overall we want to reduce the maintenance cost which is: `TP*(Repair cost) + FN*(Replacement cost) + FP*(Inspection cost)`\n",
    "- In simple language, we can say that we are assigning different weightage to the different types of predictions.\n",
    "- Do we have any such metric which can help us minimize the maintenance cost? i.e., help us reduce FP and FN as per the weightage - We don't have any such inbuilt metric.\n",
    "- In sklearn, we can define **custom scorers** also as per our need.\n",
    "- For this classification problem, we need to reduce the maintenance cost, which can be reiterated as:\n",
    "  - Minimize (maintenance cost)\n",
    "  - Maximize (1/maintenance cost)\n",
    "  - Maximize (minimum possible maintenance cost/maintenance cost)\n",
    "- Eventually, all 3 metrics will do the same work in the backend and the only difference will be in the scale of the values of the metric.\n",
    "\n",
    "- The metric provided in the next cell is to `maximize(minimum possible maintenance cost/maintenance cost)`\n",
    "- You can modify the metric as per convenience and use it further to tune the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "X09SzkBA4jas"
   },
   "outputs": [],
   "source": [
    "# defining metric to be used for optimization and with cross-validation\n",
    "def Minimum_Vs_Model_cost(y_train, y_pred):\n",
    "    \"\"\"\n",
    "    We want the model to optimize the maintenance cost and reduce it to the lowest possible value.\n",
    "    The lowest possible maintenance cost will be achieved when each sample is predicted correctly.\n",
    "\n",
    "    In such a scenario, the maintenance cost will be the total number of failures times the maintenance cost of replacing one generator,\n",
    "    which is given by (TP + FN) * 40 (i.e., the actual positives*40).\n",
    "    For any other scenario,\n",
    "    the maintenance cost associated with the model will be given by (TP * 15 + FP * 5 + FN * 40).\n",
    "\n",
    "    We will use the ratio of these two maintenance costs as the cost function for our model.\n",
    "    The greater the ratio, the lower the associated maintenance cost and the better the model.\n",
    "    \"\"\"\n",
    "    TP = confusion_matrix(y_train, y_pred)[1, 1]\n",
    "    FP = confusion_matrix(y_train, y_pred)[0, 1]\n",
    "    FN = confusion_matrix(y_train, y_pred)[1, 0]\n",
    "    return ((TP + FN) * 15) / (TP * 15 + FP * 5 + FN * 40)\n",
    "\n",
    "\n",
    "# A value of .80 here, will represent that the minimum maintenance cost is 80% of the maintenance cost associated with the model.\n",
    "# Since minimum maintenance cost is constant for any data, when minimum cost will become 100% of maintenance cost associated with the model\n",
    "# Model will have give the least possible maintenance cost.\n",
    "\n",
    "\n",
    "# Type of scoring used to compare parameter combinations\n",
    "scorer = metrics.make_scorer(Minimum_Vs_Model_cost, greater_is_better=True)\n",
    "\n",
    "# Higher the values, the lower the maintenance cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eqCDCbcw4jas"
   },
   "source": [
    "## Model Building with Original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "V-tpzI7g4jas"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: delayed in /Users/dwightpaganlugo/opt/anaconda3/lib/python3.8/site-packages (0.11.0b1)\n",
      "Requirement already satisfied: hiredis in /Users/dwightpaganlugo/opt/anaconda3/lib/python3.8/site-packages (from delayed) (2.0.0)\n",
      "Requirement already satisfied: redis in /Users/dwightpaganlugo/opt/anaconda3/lib/python3.8/site-packages (from delayed) (3.5.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy==1.20.3 in /Users/dwightpaganlugo/opt/anaconda3/lib/python3.8/site-packages (1.20.3)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy==1.20.3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: imbalanced-learn in /Users/dwightpaganlugo/opt/anaconda3/lib/python3.8/site-packages (0.8.1)\n",
      "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /Users/dwightpaganlugo/opt/anaconda3/lib/python3.8/site-packages (from imbalanced-learn) (0.17.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.13.3 in /Users/dwightpaganlugo/opt/anaconda3/lib/python3.8/site-packages (from imbalanced-learn) (1.20.3)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.19.1 in /Users/dwightpaganlugo/opt/anaconda3/lib/python3.8/site-packages (from imbalanced-learn) (1.6.1)\n",
      "Requirement already satisfied, skipping upgrade: scikit-learn>=0.24 in /Users/dwightpaganlugo/opt/anaconda3/lib/python3.8/site-packages (from imbalanced-learn) (1.0.1)\n",
      "Requirement already satisfied, skipping upgrade: threadpoolctl>=2.0.0 in /Users/dwightpaganlugo/opt/anaconda3/lib/python3.8/site-packages (from scikit-learn>=0.24->imbalanced-learn) (2.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U imbalanced-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oBKJaFU24jas"
   },
   "source": [
    "## Model Building with Oversampled data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "FKxnygkE4jat"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SMOTE' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-319af191d84f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Synthetic Minority Over Sampling Technique\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSMOTE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msampling_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_neighbors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mX_train_over\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_over\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_resample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'SMOTE' is not defined"
     ]
    }
   ],
   "source": [
    "# Synthetic Minority Over Sampling Technique\n",
    "sm = SMOTE(sampling_strategy=1, k_neighbors=5, random_state=1)\n",
    "X_train_over, y_train_over = sm.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uYDlbnUO4jat"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1aimb6bn4jat"
   },
   "source": [
    "## Model Building with Undersampled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "DhxfTkvu4jat"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'RandomUnderSampler' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-88fe092cde95>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Random undersampler for under sampling the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mrus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomUnderSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampling_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mX_train_un\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_un\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_resample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'RandomUnderSampler' is not defined"
     ]
    }
   ],
   "source": [
    "# Random undersampler for under sampling the data\n",
    "rus = RandomUnderSampler(random_state=1, sampling_strategy=1)\n",
    "X_train_un, y_train_un = rus.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jROP_DVF4jau"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "50N658sB4jau"
   },
   "source": [
    "## Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PFFwX4CG4jau"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yZGY1eL84jau"
   },
   "source": [
    "## HyperparameterTuning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "czq7BZ5b4jau"
   },
   "source": [
    "- **Hyperparameter tuning can take a long time to run, so to avoid that time complexity - you can use the following grids, wherever required.**\n",
    "\n",
    "#### For XGBoost:\n",
    "param_grid={'n_estimators':np.arange(150,300,50),'scale_pos_weight':[5,10],\n",
    "            'learning_rate':[0.1,0.2], 'gamma':[0,3,5],\n",
    "            'subsample':[0.8,0.9]}\n",
    "\n",
    "#### For Gradient Boosting:\n",
    "param_grid = {\n",
    "    \"init\": [AdaBoostClassifier(random_state=1),DecisionTreeClassifier(random_state=1)],\n",
    "    \"n_estimators\": np.arange(75,150,25),\n",
    "    \"learning_rate\": [0.2, 0.05, 1],\n",
    "    \"subsample\":[0.5,0.7],\n",
    "    \"max_features\":[0.5,0.7]}\n",
    "\n",
    "\n",
    "#### For Adaboost:\n",
    "param_grid = {\n",
    "    \"n_estimators\": np.arange(10, 110, 20),\n",
    "    \"learning_rate\": [ 0.2, 0.05, 1],\n",
    "    \"base_estimator\": [\n",
    "        DecisionTreeClassifier(max_depth=1, random_state=1),\n",
    "        DecisionTreeClassifier(max_depth=2, random_state=1),\n",
    "        DecisionTreeClassifier(max_depth=3, random_state=1)]}\n",
    "\n",
    "#### For logistic Regression:\n",
    "param_grid = {'C': np.arange(0.1,1.1,0.1)}\n",
    "\n",
    "#### For Bagging Classifier:\n",
    "param_grid = {\n",
    "              'max_samples': [0.8,0.9], \n",
    "              'max_features': [0.8,0.9],\n",
    "              'n_estimators' : [40,50]}\n",
    "\n",
    "#### For Random Forest:\n",
    "param_grid = {\n",
    "    \"n_estimators\": [150,250],\n",
    "    \"min_samples_leaf\": np.arange(1, 3),\n",
    "    \"max_features\": ['sqrt','log2'],\n",
    "    \"max_samples\": np.arange(0.2, 0.6, 0.1)}\n",
    "\n",
    "#### For Decision Trees:\n",
    "param_grid = {'max_depth': np.arange(2,20), \n",
    "              'min_samples_leaf': [1, 2, 5, 7],\n",
    "              'max_leaf_nodes' : [5, 10,15],\n",
    "              'min_impurity_decrease': [0.0001,0.001]\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    " # defining a function to compute different metrics to check performance of a classification model built using sklearn\n",
    "def model_performance_classification_sklearn(model, predictors, target):\n",
    "    \"\"\"\n",
    "    Function to compute different metrics to check classification model performance\n",
    "\n",
    "    model: classifier\n",
    "    predictors: independent variables\n",
    "    target: dependent variable\n",
    "    \"\"\"\n",
    "\n",
    "    # predicting using the independent variables\n",
    "    pred = model.predict(predictors)\n",
    "\n",
    "    acc = accuracy_score(target, pred)  # to compute Accuracy\n",
    "    recall = recall_score(target, pred)  # to compute Recall\n",
    "    precision = precision_score(target, pred)  # to compute Precision\n",
    "    f1 = f1_score(target, pred)  # to compute F1-score\n",
    "\n",
    "    # creating a dataframe of metrics\n",
    "    df_perf = pd.DataFrame(\n",
    "        {\n",
    "            \"Accuracy\": acc,\n",
    "            \"Recall\": recall,\n",
    "            \"Precision\": precision,\n",
    "            \"F1\": f1,\n",
    "        },\n",
    "        index=[0],\n",
    "    )\n",
    "\n",
    "    return df_perf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def confusion_matrix_sklearn(model, predictors, target):\n",
    "    \"\"\"\n",
    "    To plot the confusion_matrix with percentages\n",
    "\n",
    "    model: classifier\n",
    "    predictors: independent variables\n",
    "    target: dependent variable\n",
    "    \"\"\"\n",
    "    y_pred = model.predict(predictors)\n",
    "    cm = confusion_matrix(target, y_pred)\n",
    "    labels = np.asarray(\n",
    "        [\n",
    "            [\"{0:0.0f}\".format(item) + \"\\n{0:.2%}\".format(item / cm.flatten().sum())]\n",
    "            for item in cm.flatten()\n",
    "        ]\n",
    "    ).reshape(2, 2)\n",
    "\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.heatmap(cm, annot=labels, fmt=\"\")\n",
    "    plt.ylabel(\"True label\")\n",
    "    plt.xlabel(\"Predicted label\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "tVZcJ0hv4jau"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-3763cea3e370>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Fitting parameters in GridSeachCV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mgrid_cv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m print(\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    800\u001b[0m         \u001b[0mfit_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_fit_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    801\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 802\u001b[0;31m         \u001b[0mcv_orig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_cv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    803\u001b[0m         \u001b[0mn_splits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv_orig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_n_splits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    804\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36mcheck_cv\u001b[0;34m(cv, y, classifier)\u001b[0m\n\u001b[1;32m   2301\u001b[0m             \u001b[0mclassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2302\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2303\u001b[0;31m             \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"multiclass\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2304\u001b[0m         ):\n\u001b[1;32m   2305\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mStratifiedKFold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/multiclass.py\u001b[0m in \u001b[0;36mtype_of_target\u001b[0;34m(y)\u001b[0m\n\u001b[1;32m    323\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"f\"\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0;31m# [.1, .2, 3] or [[.1, .2, 3]] or [[1., .2]] and not [1., 2., 3.]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m         \u001b[0m_assert_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m\"continuous\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msuffix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype)\u001b[0m\n\u001b[1;32m    112\u001b[0m         ):\n\u001b[1;32m    113\u001b[0m             \u001b[0mtype_err\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"infinity\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mallow_nan\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"NaN, infinity\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    115\u001b[0m                 msg_err.format(\n\u001b[1;32m    116\u001b[0m                     \u001b[0mtype_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg_dtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mmsg_dtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "# Creating pipeline\n",
    "model = DecisionTreeClassifier(random_state=1)\n",
    "\n",
    "# Parameter grid to pass in GridSearchCV\n",
    "param_grid = {'max_depth': np.arange(2,20),'min_samples_leaf': [1, 2, 5, 7],'max_leaf_nodes':[5, 10,15], \n",
    "    'min_impurity_decrease': [0.0001,0.001] \n",
    "}\n",
    "\n",
    "# Type of scoring used to compare parameter combinations\n",
    "scorer = metrics.make_scorer(metrics.recall_score)\n",
    "\n",
    "# Calling GridSearchCV\n",
    "grid_cv = GridSearchCV(estimator=model, param_grid=param_grid, scoring=scorer, cv=5)\n",
    "\n",
    "# Fitting parameters in GridSeachCV\n",
    "grid_cv.fit(X_train, y_train)\n",
    "\n",
    "print(\n",
    "    \"Best Parameters:{} \\nScore: {}\".format(grid_cv.best_params_, grid_cv.best_score_)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D9JNnpxa4jau"
   },
   "source": [
    "## Model Performance comparison and choosing the final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0JG85rkY4jav"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d_pDMFAz4jav"
   },
   "source": [
    "## Test set final performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g8-epsXv4jav"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TM6VZTRn4jav"
   },
   "source": [
    "## Pipelines to build the final model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zzg12gvx4jav"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c5hPmHyR4jaw"
   },
   "source": [
    "# Business Insights and Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FnBbg6sH4jaw"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Learner's_notebook.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
